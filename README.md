# llum

### A fast, light, open chat UI

![image](https://github.com/user-attachments/assets/38cc47cf-06a3-4dca-8ee5-d9c9edf57903)

### Key features:

- ðŸ”Œ Multiple providers, plug in your API keys (stored entirely locally) and you're good to go

  - Local models (through Ollama)
  - OpenRouter (which lets you use ALL models across many providers: OpenAI, Anthropic, OSS, 50+ others)
  - OpenAI
  - Anthropic
  - Mistral
  - Groq

- ðŸ› ï¸ Tool use
  - Check out `server/toolfns/toolfns.go`. You only need to write functions. The function comment is the description the model receives, so it knows what to use. Click the `Sync` button in the web UI to refresh your tools.
- ðŸ–¼ï¸ Multimodal input: upload, paste, or share links to images
- ðŸŽ¨ Image generation using DALL-E 3
- ðŸ“ Multi-shot prompting. Also edit, delete, regenerate messages, whatever. The world is your oyster
- âš¡ Pre-filled responses (where supported by provider)
- ðŸŒ Support for all available models across all providers
- ðŸ”„ Change model mid-conversation
- ðŸ” Sync chats and keys across devices, end-to-end encrypted. Self-hosted, or use our hosted instance.
- ðŸ”— Conversation sharing (if you choose to share, your conversation has to be stored on an external server for the share link to be made available. Self-hosted share options coming soon. No, I will not view any of your stuff.)
- ðŸŒ¿ Branching conversation history (like the left-right ChatGPT arrows that you can click to go back to a previous response)

### Privacy:

- Completely private and transparent. All your conversation history and keys are stored entirely locally, and kept only in your browser, on your device.

## How to install?

If you don't want to use tools, you don't need to install anything. A hosted instance is available at: https://llum.chat

If you want to use tools, proceed below.

## Single binary:

The llum tool server is available prebuilt as a single binary. [Download prebuilt package from the releases page.](https://github.com/zakkor/llum/releases)

Download the binary for your platform, then run it, which will start the tool server:

```
./llum-darwin-amd64
Tool server running at http://localhost:8081
```

Go back to https://llum.chat, head over to Settings -> Tool calling, and click the "Refresh tools" button. You should be good to go!

### Building client and server locally:

1. Clone the repository
2. Install and start the client: `npm i && npm run dev`. The client will be accessible at http://localhost:5173
3. Install and start the server: `cd server && go generate ./... && go build && ./server -password foobar`. The server will be accessible at http://localhost:8081. You can plug this into the server address in the chat UI along with the password you selected.
